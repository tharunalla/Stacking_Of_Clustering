{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "K-Means Clustering"
      ],
      "metadata": {
        "id": "sdRvEnxzffqq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ex8vCXZCDE1-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7288bf62-aa38-4720-8b4b-137dc7d5f9ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Score (k=10): 0.5643016818392292\n",
            "Davies-Bouldin Index: 0.8411646958445402\n",
            "Calinski-Harabasz Index: 98.14582721500408\n",
            "Updated dataset saved to Updated_Clustering_with_Labels.xlsx\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "\n",
        "# Load the data\n",
        "def load_data(file_path):\n",
        "    try:\n",
        "        data = pd.read_excel(file_path)\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return None\n",
        "\n",
        "# Apply K-means clustering\n",
        "def apply_kmeans(data, n_clusters=10):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    kmeans.fit(data)\n",
        "    return kmeans, kmeans.labels_\n",
        "\n",
        "# Evaluate clustering performance using all metrics\n",
        "def evaluate_clustering(data, labels):\n",
        "    silhouette_avg = silhouette_score(data, labels)  # Silhouette score\n",
        "    davies_bouldin = davies_bouldin_score(data, labels)  # Davies-Bouldin index\n",
        "    calinski_harabasz = calinski_harabasz_score(data, labels)  # Calinski-Harabasz index\n",
        "\n",
        "    return silhouette_avg, davies_bouldin, calinski_harabasz\n",
        "\n",
        "# Main function to run the process and update the existing Excel file\n",
        "def main(file_path, n_clusters=10):\n",
        "    # Load the data\n",
        "    data = load_data(file_path)\n",
        "    if data is None:\n",
        "        return  # Exit if there was an error loading the data\n",
        "\n",
        "    # Select only the first 196 columns\n",
        "    if data.shape[1] < 196:\n",
        "        print(\"The dataset has fewer than 196 columns. Please check the input data.\")\n",
        "        return\n",
        "\n",
        "    data_values = data.iloc[:, :196]  # Select first 196 columns\n",
        "\n",
        "    # Apply K-means clustering\n",
        "    kmeans_model, labels = apply_kmeans(data_values, n_clusters)\n",
        "\n",
        "    # Evaluate performance\n",
        "    silhouette_avg, davies_bouldin, calinski_harabasz = evaluate_clustering(data_values, labels)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Silhouette Score (k={n_clusters}): {silhouette_avg}\")\n",
        "    print(f\"Davies-Bouldin Index: {davies_bouldin}\")\n",
        "    print(f\"Calinski-Harabasz Index: {calinski_harabasz}\")\n",
        "\n",
        "    # Add the labels as a new column to the original data\n",
        "    data['k-means_label'] = labels\n",
        "\n",
        "    # Save the updated data back to the existing Excel file\n",
        "    try:\n",
        "        data.to_excel(file_path, index=False)  # Overwrite the existing file\n",
        "        print(f\"Updated dataset saved to {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving updated dataset: {e}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "# Specify the file path and number of clusters\n",
        "file_path = 'Updated_Clustering_with_Labels.xlsx'  # Path to the existing file\n",
        "n_clusters = 10\n",
        "\n",
        "# Run the main function and get the updated dataset\n",
        "updated_data = main(file_path, n_clusters)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hierarchical Clustering"
      ],
      "metadata": {
        "id": "56FB2aTEfh9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "import numpy as np\n",
        "\n",
        "# Module 1: Load the data\n",
        "def load_data(file_path):\n",
        "    data = pd.read_excel(file_path)\n",
        "    return data\n",
        "\n",
        "# Module 2: Apply Hierarchical (Agglomerative) Clustering\n",
        "def apply_hierarchical_clustering(data, n_clusters=10):\n",
        "    clustering = AgglomerativeClustering(n_clusters=n_clusters)\n",
        "    labels = clustering.fit_predict(data)\n",
        "    return clustering, labels\n",
        "\n",
        "# Module 4: Calculate Centroids for Clusters\n",
        "def calculate_centroids(data, labels):\n",
        "    unique_labels = np.unique(labels)\n",
        "    centroids = np.array([data[labels == label].mean(axis=0) for label in unique_labels])\n",
        "    return centroids\n",
        "\n",
        "# Module 6: Evaluate clustering performance using all metrics\n",
        "def evaluate_clustering(data, labels):\n",
        "    silhouette_avg = silhouette_score(data, labels)  # Silhouette score\n",
        "    davies_bouldin = davies_bouldin_score(data, labels)  # Davies-Bouldin index\n",
        "    calinski_harabasz = calinski_harabasz_score(data, labels)  # Calinski-Harabasz index\n",
        "\n",
        "    return silhouette_avg, davies_bouldin, calinski_harabasz\n",
        "\n",
        "# Main function to run the process and update the existing Excel file\n",
        "def main(file_path, n_clusters):\n",
        "    # Load the data\n",
        "    data = load_data(file_path)\n",
        "\n",
        "    # Select only the first 195 columns\n",
        "    data = data.iloc[:, :196]\n",
        "\n",
        "    # Apply Hierarchical clustering\n",
        "    clustering_model, labels = apply_hierarchical_clustering(data, n_clusters)\n",
        "\n",
        "    # Evaluate performance\n",
        "    silhouette_avg, davies_bouldin, calinski_harabasz = evaluate_clustering(data, labels)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Silhouette Score (Hierarchical Clustering): {silhouette_avg}\")\n",
        "    print(f\"Davies-Bouldin Index: {davies_bouldin}\")\n",
        "    print(f\"Calinski-Harabasz Index: {calinski_harabasz}\")\n",
        "\n",
        "    # Add the labels as a new column to the original data\n",
        "    data['Hierarchical_Clustering_label'] = labels\n",
        "\n",
        "    # Save the updated dataset back to the existing Excel file\n",
        "    data.to_excel(file_path, index=False)  # Overwrite the existing file\n",
        "    print(f\"Updated dataset saved to {file_path}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "# Specify the file path and number of clusters\n",
        "file_path = 'Updated_Clustering_with_Labels.xlsx'  # Path to the existing file\n",
        "n_clusters = 10\n",
        "\n",
        "# Run the main function and get the updated dataset\n",
        "updated_data = main(file_path, n_clusters)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulzVXN1UfiZ0",
        "outputId": "89218311-96bc-45a5-d919-fbce5ba639bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Score (Hierarchical Clustering): 0.5767027381927193\n",
            "Davies-Bouldin Index: 0.852873153751695\n",
            "Calinski-Harabasz Index: 100.83198234709431\n",
            "Updated dataset saved to Updated_Clustering_with_Labels.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gaussian Mixture Model (GMM)"
      ],
      "metadata": {
        "id": "DEqgZQAKfi-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from sklearn.preprocessing import StandardScaler  # Optional: for scaling\n",
        "\n",
        "# Load the dataset\n",
        "def load_data(file_path, sheet_name=0):\n",
        "    return pd.read_excel(file_path, sheet_name=sheet_name)\n",
        "\n",
        "# Apply GMM clustering\n",
        "def apply_gmm(data, n_components=10, covariance_type='full'):\n",
        "    gmm = GaussianMixture(n_components=n_components, covariance_type=covariance_type, random_state=42)\n",
        "    labels = gmm.fit_predict(data)\n",
        "    return labels, gmm\n",
        "\n",
        "# Evaluate clustering performance using three metrics\n",
        "def evaluate_clustering(data, labels):\n",
        "    if len(set(labels)) > 1:\n",
        "        silhouette = silhouette_score(data, labels)  # Silhouette score\n",
        "        davies_bouldin = davies_bouldin_score(data, labels)  # Davies-Bouldin index\n",
        "        calinski_harabasz = calinski_harabasz_score(data, labels)  # Calinski-Harabasz index\n",
        "    else:\n",
        "        silhouette = -1  # Invalid silhouette score for 1 cluster\n",
        "        davies_bouldin = -1\n",
        "        calinski_harabasz = -1\n",
        "\n",
        "    return silhouette, davies_bouldin, calinski_harabasz\n",
        "\n",
        "# Main function to execute GMM on the loaded data and update the existing Excel file\n",
        "def run_gmm_clustering(file_path, sheet_name=0, n_components=10, covariance_type='full'):\n",
        "    # Load data\n",
        "    data = load_data(file_path, sheet_name)\n",
        "\n",
        "    # Select only the first 195 columns and drop rows with NaNs\n",
        "    data_values = data.iloc[:, :196].dropna().values\n",
        "\n",
        "    # Optional: Scale the data\n",
        "    # scaler = StandardScaler()\n",
        "    # data_values = scaler.fit_transform(data_values)\n",
        "\n",
        "    # Apply GMM\n",
        "    labels, gmm = apply_gmm(data_values, n_components=n_components, covariance_type=covariance_type)\n",
        "\n",
        "    # Evaluate performance\n",
        "    silhouette, davies_bouldin, calinski_harabasz = evaluate_clustering(data_values, labels)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Silhouette Score: {silhouette}\")\n",
        "    print(f\"Davies-Bouldin Index: {davies_bouldin}\")\n",
        "    print(f\"Calinski-Harabasz Index: {calinski_harabasz}\")\n",
        "\n",
        "    # Add the labels as a new column to the original dataset\n",
        "    data['Gaussian_Mixture_Model_(GMM)_Clustering_label'] = labels\n",
        "\n",
        "    # Save the updated dataset back to the existing Excel file\n",
        "    data.to_excel(file_path, index=False)  # Overwrite the existing file\n",
        "    print(f\"Updated dataset saved to {file_path}\")\n",
        "\n",
        "# Specify the file path and run the GMM clustering with n_components=10\n",
        "file_path = 'Updated_Clustering_with_Labels.xlsx'  # Path to the existing file\n",
        "run_gmm_clustering(file_path, n_components=10, covariance_type='full')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDIIXqtTfjaV",
        "outputId": "75c3023e-32ee-40b3-ec8b-badf97cd9c31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Score: 0.5643016818392292\n",
            "Davies-Bouldin Index: 0.8411646958445402\n",
            "Calinski-Harabasz Index: 98.14582721500408\n",
            "Updated dataset saved to Updated_Clustering_with_Labels.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn-extra"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiZX8VpNh8cP",
        "outputId": "a80eea72-3a03-4d9e-a96e-ab9c0b6dc153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn-extra in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.26.4)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.5.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Medoids Clustering"
      ],
      "metadata": {
        "id": "X1tQT52lfj5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "\n",
        "# Load the data\n",
        "def load_data(file_path):\n",
        "    try:\n",
        "        data = pd.read_excel(file_path)\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return None\n",
        "\n",
        "# Apply K-Medoids clustering\n",
        "def apply_kmedoids(data, n_clusters=10, metric='euclidean'):\n",
        "    kmedoids = KMedoids(n_clusters=n_clusters, metric=metric, random_state=42)\n",
        "    kmedoids.fit(data)\n",
        "    return kmedoids, kmedoids.labels_\n",
        "\n",
        "# Evaluate clustering performance using all metrics\n",
        "def evaluate_clustering(data, labels):\n",
        "    silhouette_avg = silhouette_score(data, labels)  # Silhouette score\n",
        "    davies_bouldin = davies_bouldin_score(data, labels)  # Davies-Bouldin index\n",
        "    calinski_harabasz = calinski_harabasz_score(data, labels)  # Calinski-Harabasz index\n",
        "\n",
        "    return silhouette_avg, davies_bouldin, calinski_harabasz\n",
        "\n",
        "# Main function to run the process and update the existing Excel file\n",
        "def main(file_path, n_clusters=10, metric='euclidean'):\n",
        "    # Load the data\n",
        "    data = load_data(file_path)\n",
        "    if data is None:\n",
        "        return  # Exit if there was an error loading the data\n",
        "\n",
        "    # Select only the first 196 columns\n",
        "    if data.shape[1] < 196:\n",
        "        print(\"The dataset has fewer than 196 columns. Please check the input data.\")\n",
        "        return\n",
        "\n",
        "    data_values = data.iloc[:, :196]  # Select first 196 columns\n",
        "\n",
        "    # Apply K-Medoids clustering\n",
        "    kmedoids_model, labels = apply_kmedoids(data_values, n_clusters, metric)\n",
        "\n",
        "    # Evaluate performance\n",
        "    silhouette_avg, davies_bouldin, calinski_harabasz = evaluate_clustering(data_values, labels)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Silhouette Score (k={n_clusters}): {silhouette_avg}\")\n",
        "    print(f\"Davies-Bouldin Index: {davies_bouldin}\")\n",
        "    print(f\"Calinski-Harabasz Index: {calinski_harabasz}\")\n",
        "\n",
        "    # Add the labels as a new column to the original data\n",
        "    data['k-medoids_label'] = labels\n",
        "\n",
        "    # Save the updated data back to the existing Excel file\n",
        "    try:\n",
        "        data.to_excel(file_path, index=False)  # Overwrite the existing file\n",
        "        print(f\"Updated dataset saved to {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving updated dataset: {e}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "# Specify the file path and number of clusters\n",
        "file_path = 'Updated_Clustering_with_Labels.xlsx'  # Path to the existing file\n",
        "n_clusters = 10\n",
        "\n",
        "# Run the main function and get the updated dataset\n",
        "updated_data = main(file_path, n_clusters)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5ImBMKGfkTp",
        "outputId": "caf6f6d3-479d-4b5c-dc92-13de5fe02e5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Score (k=10): 0.20790050906770893\n",
            "Davies-Bouldin Index: 1.1259539660727949\n",
            "Calinski-Harabasz Index: 30.790791260647566\n",
            "Updated dataset saved to Updated_Clustering_with_Labels.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Diagonal Covariance GMM"
      ],
      "metadata": {
        "id": "uJ1xKFigfk1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from sklearn.preprocessing import StandardScaler  # Optional: for scaling\n",
        "\n",
        "# Load the dataset\n",
        "def load_data(file_path, sheet_name=0):\n",
        "    return pd.read_excel(file_path, sheet_name=sheet_name)\n",
        "\n",
        "# Apply GMM clustering\n",
        "def apply_gmm(data, n_components=10, covariance_type='diag'):\n",
        "    gmm = GaussianMixture(n_components=n_components, covariance_type=covariance_type, random_state=42)\n",
        "    labels = gmm.fit_predict(data)\n",
        "    return labels, gmm\n",
        "\n",
        "# Evaluate clustering performance using three metrics\n",
        "def evaluate_clustering(data, labels):\n",
        "    if len(set(labels)) > 1:\n",
        "        silhouette = silhouette_score(data, labels)  # Silhouette score\n",
        "        davies_bouldin = davies_bouldin_score(data, labels)  # Davies-Bouldin index\n",
        "        calinski_harabasz = calinski_harabasz_score(data, labels)  # Calinski-Harabasz index\n",
        "    else:\n",
        "        silhouette = -1  # Invalid silhouette score for 1 cluster\n",
        "        davies_bouldin = -1\n",
        "        calinski_harabasz = -1\n",
        "\n",
        "    return silhouette, davies_bouldin, calinski_harabasz\n",
        "\n",
        "# Main function to execute GMM on the loaded data and update the existing Excel file\n",
        "def run_gmm_clustering(file_path, sheet_name=0, n_components=10, covariance_type='full'):\n",
        "    # Load data\n",
        "    data = load_data(file_path, sheet_name)\n",
        "\n",
        "    # Select only the first 195 columns and drop rows with NaNs\n",
        "    data_values = data.iloc[:, :196].dropna().values\n",
        "\n",
        "    # Optional: Scale the data\n",
        "    # scaler = StandardScaler()\n",
        "    # data_values = scaler.fit_transform(data_values)\n",
        "\n",
        "    # Apply GMM\n",
        "    labels, gmm = apply_gmm(data_values, n_components=n_components, covariance_type=covariance_type)\n",
        "\n",
        "    # Evaluate performance\n",
        "    silhouette, davies_bouldin, calinski_harabasz = evaluate_clustering(data_values, labels)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Silhouette Score: {silhouette}\")\n",
        "    print(f\"Davies-Bouldin Index: {davies_bouldin}\")\n",
        "    print(f\"Calinski-Harabasz Index: {calinski_harabasz}\")\n",
        "\n",
        "    # Add the labels as a new column to the original dataset\n",
        "    data['Diag_Gaussian_Mixture_Model_(GMM)_Clustering_label'] = labels\n",
        "\n",
        "    # Save the updated dataset back to the existing Excel file\n",
        "    data.to_excel(file_path, index=False)  # Overwrite the existing file\n",
        "    print(f\"Updated dataset saved to {file_path}\")\n",
        "\n",
        "# Specify the file path and run the GMM clustering with n_components=10\n",
        "file_path = 'Updated_Clustering_with_Labels.xlsx'  # Path to the existing file\n",
        "run_gmm_clustering(file_path, n_components=10, covariance_type='diag')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaCIECUBflQX",
        "outputId": "519ceb0b-790a-4389-d1e6-c77d2c169d4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Score: 0.5643016818392292\n",
            "Davies-Bouldin Index: 0.8411646958445402\n",
            "Calinski-Harabasz Index: 98.14582721500408\n",
            "Updated dataset saved to Updated_Clustering_with_Labels.xlsx\n"
          ]
        }
      ]
    }
  ]
}